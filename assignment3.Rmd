---
title: "IDA Assignment 3"
author: "Johnny Lee, s1687781"
date: "8th April 2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setlocale("LC_ALL", "English")
library(maxLik)
load("dataex2.Rdata")
load("dataex4.Rdata")
load("NHANES2.Rdata")
```

# Q1.
Consider the \texttt{nhanes} dataset in mice. For more information please type help(\texttt{nhanes}) in the R console.

## a)

Show that the maximum likelihood estimator based on the observed data
$\{(x_i,r_i)\}^{n}_{i=1}$ is given by
$$\hat{\theta} = \frac{\sum_{i=1}^n X_i^2}{2\sum_{i=1}^nR_i}.$$

### **Answer** :

We first define the Survival function (from **Workshop 3**)as
$$S(C;\theta)=\mathbb{P}(Y_i>C;\theta)=1-F(y_i;\theta)$$ which also
represents the censored observations. For the uncensored observations,
we have
$$f(y_i;\theta)=\frac{d}{dy_i}F(y_i;\theta)=\frac{ye^{-y^2/2\theta}}{\theta}$$
Given that $Y_1,\dots,Y_n$ are independent and identically distributed,
we have the likelihood function as,
\begin{equation}\label{eq:likelihood}
  \begin{split}
    L(\theta|\boldsymbol{y,r})&= \prod^{n}_{i=1} \bigg( [f(y_i;\theta)]^{r_i} [S(C;\theta)]^{1-r_i}\bigg)\\
    &= \prod^{n}_{i=1} \bigg( [\frac{y_i e^{-y_i^2/2\theta}}{\theta}]^{r_i} [e^{-C^2/\theta}]^{1-r_i}\bigg)\\
    &= \bigg(\prod^{n}_{i=1}y_i^{r_i}\bigg)\bigg(\frac{1}{\theta}\bigg)^{\sum^{n}_{i=1}r_i}\exp\bigg(\frac{\sum^{n}_{i=1}(r_i y^2_i +(1-r_i)C^2)}{2\theta}\bigg)
  \end{split}
\end{equation}

Now we can rewrite the expression inside the exponential in terms of $X_i$. $X_i$ can be expressed as $$x_i=r_iy_i+C(1-r_i)$$ Then by taking square on both sides we have,
$$x_i^2=r_i^2y_i^2+(1-r_i)^2C^2+2r_iy_iC(1-r_i)$$ Noting that $R_i$ is
binary, we can then conclude with the expression as
\begin{equation} \label{eq:x}
  \begin{split}
    x_i^2=r_iy_i^2+(1-r_i)C^2
  \end{split}
\end{equation}

Now we substitute (\ref{eq:x}) into (\ref{eq:likelihood}) to have,
\begin{equation}\label{eq:loglik}
  \begin{split}
    L(\theta|\boldsymbol{y,r}) &= \bigg(\prod^{n}_{i=1}y_i^{r_i}\bigg)\bigg(\frac{1}{\theta}\bigg)^{\sum^{n}_{i=1}r_i}\exp\bigg(\frac{\sum^{n}_{i=1}(r_i y^2_i +(1-r_i)C^2)}{2\theta}\bigg)\\
    \implies \log(L(\theta|\boldsymbol{y,r}))&=\sum^{n}_{i=1}r_i\log(y_i)-\log\theta\sum^{n}_{i=1}r_i-\frac{\sum^{n}_{i=1}x_i^2}{2\theta}\\
    \implies \frac{d}{d\theta}\log L(\theta|\boldsymbol{y,r})&=\frac{1}{\theta}\sum^{n}_{i=1}r_i + \frac{1}{2\theta^2}\sum^{n}_{i=1}x_i^2
    \end{split}
\end{equation} 
By equating the derivative (\ref{eq:loglik}) to $0$, we can obtain the
maximum likelihood estimate of $\theta$ as below.
$$\hat{\theta}_{MLE} = \frac{\sum_{i=1}^n x_i^2}{2\sum_{i=1}^nr_i} \quad \text{(shown)}$$.

\newpage

## b)

Show that the expected Fisher Information for the observed data
likelihood is $$I(\theta) = \frac{n}{\theta^2}(1 - e^{-C^2/(2\theta)})$$

**Note:**
$\int_0^Cy^2f(y;\theta)dy = -C^2e^{-C^2/(2\theta)} + 2\theta(1 - e^{-C^2 / (2\theta)})$,
where $f(y;\theta)$ is the density function corresponding to the
cumulative distribution function $F(y;\theta)$ defined above.

### **Answer** :

From (\ref{eq:loglik}), we take another derivative of it and thus obtain
as below
$$\frac{d^2}{d\theta^2}\log L(\theta)=\frac{1}{\theta^2}\sum^{n}_{i=1}r_i-\frac{x_i^2}{\theta^3}$$
Then, the Fisher Information for the observed data likelihood is,
\begin{equation}\label{eq:fi}
  \begin{split}
    I(\theta)&=-\mathbb{E}\bigg(\frac{\sum^{n}_{i=1}r_i}{\theta^2}-\frac{x^2_i}{\theta^3}\bigg)\\
    &=-\frac{n\mathbb{E}(R)}{\theta^2}+\frac{n\mathbb{E}(X^2)}{\theta^3}\\
    &=-\frac{n\mathbb{E}(R)}{\theta^2}+\frac{1}{\theta^3}\bigg(n\mathbb{E}(RY^2)+nC^2\mathbb{E}(1-R)\bigg)
  \end{split}
\end{equation}

Again, noting that $R_i$ is binary, \begin{equation}\label{eq:expR}
  \begin{split}
    \mathbb{E}(R)&=1\cdot\mathbb{P}(R=1)+0\cdot\mathbb{P}(R=0)\\
    &=\mathbb{P}(R=1)=\mathbb{P}(Y\leq C)\\
    &=F(C;\theta) = 1-e^{-C^2/2\theta} \\
    \implies \mathbb{E}(1-R)&=e^{-C^2/2\theta}
  \end{split}
\end{equation}

With the given equation,
$\mathbb{E}(RY^2)=\int_0^Cy^2f(y;\theta)dy = -C^2e^{-C^2/(2\theta)} + 2\theta(1 - e^{-C^2 / (2\theta)})$,
we can combine all the above equations as express the expected Fisher
Information again,

\begin{equation}
  \begin{split}
    I(\theta)&=\frac{n\mathbb{E}(R)}{\theta^2}+\frac{1}{\theta^3}\bigg(n\mathbb{E}(RY^2)+nC^2\mathbb{E}(1-R)\bigg)\\
    &=\frac{-n}{\theta^2}(1-e^{-C^2/2\theta})-\frac{n}{\theta^3}(C^2e^{-C^2/2\theta})+\frac{n}{\theta^3}(2\theta(1-e^{-C^2/2\theta}))+\frac{n}{\theta^3}(C^2e^{-C^2/2\theta}) \\
    &=\frac{n}{\theta^2}(1-e^{-C^2/2\theta}) \quad \text{(shown)}
  \end{split}
\end{equation}

\newpage

## c)

Appealing to the asymptotic normality of the maximum likelihood
estimator, provide a $95\%$ confidence interval for $\theta$.

### **Answer** :

By the Central Limit Theorem, asymptotic normality of the maximum likelihood estimator is given as, 
<!-- $$\sqrt{n}(\hat{\theta}_{MLE}-\theta)\sim N_p(0, I(\theta)^{-1})$$.  -->
$$\hat{\theta}_{MLE}\sim N_p(\theta, I(\theta)^{-1})$$
Thus, with $0$ and $\frac{1}{I(\theta)}$ as the asymptotic mean and variance
respectively, we can obtain the $95\%$ confidence interval as below,
  $$\hat{\theta}_{MLE}\pm \frac{1.96}{\sqrt{I(\theta)}}=\hat{\theta}_{MLE}\pm\frac{1.96\cdot\theta_{MLE}}{\sqrt{n(1-e^{-C^2/2\theta_{MLE}})}}$$
  <!-- $$\hat{\theta}_{MLE}\pm \frac{1.96}{\sqrt{nI(\theta)}}=\hat{\theta}_{MLE}\pm\frac{1.96\cdot\theta_{MLE}}{\sqrt{n^2(1-e^{-C^2/2\theta_{MLE}})}}$$ -->

\newpage

# Q2.

Suppose that a dataset consists of $100$ subjects and $10$ variables.
Each variable contains $10\%$ of missing values. What is the largest
possible subsample under a complete case analysis? What is the smallest?
Justify.

Suppose that $Y_i \overset{\text{iid}}{\sim} N(\mu, \sigma^2)$ are iid
for $i=1,...,n$. Further suppose that now observations are (left)
censored if $Y_i < D$, for some known $D$ and let

\begin{equation*}
  X_i=
  \begin{cases}
    Y_i \quad \text{if} \; Y_i \geq D,\\
    D \quad \text{if} \; Y_i < D, 
  \end{cases}
  \quad
  R_i=
  \begin{cases}
    1 \quad \text{if} \; Y_i \geq D\\
    0 \quad \text{if} \; Y_i < D
  \end{cases}
\end{equation*}

## a)

Show that the log-likelihood of the observed data
$\{(x_i,r_i)\}^{n}_{i=1}$ is given by

$$\log L(\mu, \sigma^2|\boldsymbol{x}, \boldsymbol{r}) = \sum_{i=1}^n\left\{r_i\log\phi(x_i; \mu, \sigma^2) + (1-r_i)\log\Phi(x_i;\mu,     \sigma^2)\right\}$$

where $\phi(\cdot;\mu,\sigma^2)$ and $\Phi(\cdot;\mu, \sigma^2)$ stands,
respectively, for the density function and cumulative distribution
function of the normal distribution with mean $\mu$ and variance
$\sigma^2$.

### **Answer** :

We first define the case for censored observations (from **Workshop 3**)as 
$$\mathbb{P}(Y_i<D;\mu,\sigma^2)=F(D;\mu,\sigma^2)=\Phi(x_i;\mu,\sigma^2)$$
For the uncensored observation, it is the density function and we have 
$$\phi(x_i;\mu,\sigma^2)$$

Given that $X_1,\dots,X_n$ are independent and identically distributed,
we have the likelihood function as,


\begin{equation}
  \begin{split}
    L(\mu,\sigma^2|\boldsymbol{x,r}) &= \prod^{n}_{i=1}\bigg([\phi(x_i;\mu,\sigma^2)]^{r_i}[\Phi(x_i;\mu,\sigma^2)]^{1-r_i}\bigg)\\
    \implies l(\mu, \sigma^2|\boldsymbol{x}, \boldsymbol{r})&=\log \prod_{i=1}^{n}\bigg(\phi(x_i;\mu,\sigma^2)]^{r_i}[\Phi(x_i;\mu, \sigma^2)]^{1-r_i}\bigg)\\
    &=  \sum_{i=1}^n\bigg(r_i\log\phi(x_i; \mu, \sigma^2) + (1-r_i)\log\Phi(x_i;\mu, \sigma^2)\bigg) \quad \text{(shown)}
  \end{split}
\end{equation}

\newpage

## b)

Determine the maximum likelihood estimate of $\mu$ based on the data
available in the file `dataex2.Rdata`. Consider $\sigma^2$ known and
equal to $1.5^2$. **Note**: You can use a built in function such as
`optim` or the `maxLik` package in your implementation.

### **Answer** :

```{r}

```

We built a function `log.lik()` that produces the log likelihood and then used `maxLik()` to simulate $\mu$ based on the data. With Newton-Raphson method, we estimated $\hat{\mu}=5.5328$ and standard error of $0.1075$

\newpage

# Q3.

Consider a bivariate normal sample $(Y_1, Y_2)$ with parameters
$\theta=(\mu_1,\mu_2,\sigma_1^2,\sigma_{12},\sigma_2^2)$. The variable
$Y_1$ is fully observed, while some values of $Y_2$ are missing. Let $R$
be the missingness indicator, taking the value 1 for observed values and
0 for missing values. For the following missing data mechanisms state,
justifying, whether they are ignorable for likelihood-based estimation.

## a)

$\text{logit}\{\mathbb{P}(R=0|y_1,y_2,\theta,\psi)\}=\psi_0+\psi_1y_1, \quad \psi= (\psi_0,\psi_1)$
distinct from $\theta$.

### **Answer** :

Referring to the ignorability assumption (from **Lecture 6.1**), the missing in $Y_2$ is either **MAR** or **MCAR** and its model parameters, $\theta = (\mu_1, \mu_2, \sigma_1^2, \sigma_{12}, \sigma_2^2)$ and missing mechanism parameter, $\psi$.

First, the missing mechanism is **MAR**. This is because the missingness is only dependent on $Y_1$ which is a fully observed variable. The parameters, $\{\theta,\psi\}$ are also distinct. Therefore, the ignorability assumption holds here and (a) is ignorable for likelihood-based estimation.

## b)

$\text{logit}\{\mathbb{P}(R=0|y_1,y_2,\theta,\psi)\}=\psi_0+\psi_1y_2, \quad \psi= (\psi_0,\psi_1)$
distinct from $\theta$.

### **Answer** :

The missing mechanism is **MNAR** as the mechanism is only dependent on $Y_2$. Therefore, the missing value is depending on itself and possibly other factors. Hence, by referring to the ignorability assumption (from **Lecture 6.1**), we conclude that (b) is not ignorable for likelihood-based estimation.

### c)

$\text{logit}\{\mathbb{P}(R=0|y_1,y_2,\theta,\psi)\}=0.5(\mu_1+\psi y_1)$,
scalar $\psi$ distinct from $\theta$.

### **Answer** :

The missing mechanism here is dependent on both $\mu_1$ and $Y_1$ thus **MAR**. We can observe similarity to (a). Distinctness of the parameters means that the parameter space of $\{\theta, \psi\}$ is equal to the Cartesian product of their individual product spaces. However, the $\mu_1$ also exists in the parameter space. This violates the ignorability assumption. Hence, (c) is not ignorable for likelihood-based estimation.

\newpage

# Q4.

$$Y_i\overset{\text{ind.}}{\sim} \text{Bernoulli}(p_i(\boldsymbol{\beta}))$$

$$p_i(\boldsymbol{\beta}) = \frac{exp(\beta_0 + x_i\beta_1)}{1 + exp(\beta_0 + x_i\beta_1)},$$
for $i=1,\cdots,n$ and $\boldsymbol{\beta} = (\beta_0, \beta_1)'$.
Although the covariate $x$ is fully observed, the response variable $Y$
has missing values. Assuming ignorability, derive and implement the EM
algorithm to compute the MLE of $\boldsymbol{\beta}$ based on the data
available in `dataex4.Rdata`. **Note**: $1$) For simplicity, and without
loss of generality because we have a univariate pattern of missingness,
when writing down your expressions, you can assume that the first $m$
values of $Y$ are observed and the remaining $n-m$ are missing. $2$) You
can use a built in function such as `optim` or the `maxLik` package for
the M-step.

### **Answer** :

```{r}
head(dataex4)
cat("Number of missing values in Y:", sum(is.na(dataex4)))
```
Scrutinising on the dataset, we can observe that the missing value only occurs in $Y$ and there are $95$ missing values occurring in a univariate pattern

We first derive the likelihood function to implement the EM algorithm given that $\boldsymbol{y_{obs}}=y_1,\dots,y_m$ and $\boldsymbol{y_{mis}}=y_{m+1},\dots,y_{n}$.

\begin{equation}
  \begin{split}
    L(\beta_0, \beta_1|\boldsymbol{x,y_{obs},y_{mis}}) &= \prod^{n}_{i=1} \bigg([p_i(\beta_0,\beta_1)]^{y_i}[1-p(\beta_0,\beta_1)]^{1-y_i}\bigg)\\
    &= \prod^{n}_{i=1}\bigg( \frac{e^{\beta_0+x_i\beta_1}}{1+e^{\beta_0+x_i\beta_1}} \bigg)^{y_i} \bigg(\frac{1}{1+e^{\beta_0+x_i\beta_1}}\bigg)^{1-y_i} \\
    \implies \log L(\beta_0,\beta_1|\boldsymbol{x,y_{obs},y_{mis}}) &= \sum^{n}_{i=1} \bigg(y_i\log\bigg(\frac{e^{\beta_0+x_i\beta_1}}{1+e^{\beta_0+x_i\beta_1}}\bigg)+(1-y_i)\log\bigg(\frac{1}{1+e^{\beta_0+x_i\beta_1}}\bigg) \bigg)\\
    &= \sum^{n}_{i=1}\bigg(y_i\log(e^{\beta_0+x_i\beta_1})-\log(1+e^{\beta_0+x_i\beta_1})-y_i\log(1+e^{\beta+x_i\beta_1})+y_i\log(1+e^{\beta+x_i\beta_1}) \bigg) \\
    &= \sum^{n}_{i=1} \bigg(y_i(\beta_0+x_i\beta_1)-\log(1+e^{\beta_0+x_i\beta_1}) \bigg) \\
    &= l(\boldsymbol{\beta}|\boldsymbol{x,y_{obs},y_{mis}})
  \end{split}
\end{equation}

Now we proceed to implement the EM algorithm by calculating $Q(\boldsymbol{\beta}|\boldsymbol{\beta^{(t)}})$
\begin{equation}
  \begin{split}
    Q(\boldsymbol{\beta}|\boldsymbol{\beta^{(t)}})&=\mathbb{E}_{\boldsymbol{y_{mis}}}
    [l(\boldsymbol{\beta}|\boldsymbol{x, y_{obs}, y_{mis}})|\boldsymbol{y_{obs}, x, \beta^{(t)}}] \\
    &=\sum^{m}_{i=1}\bigg(y_i(\beta_0 + x_i\beta_i)\bigg)-\sum^{n}_{i=1}\bigg(\log(1 + e^{\beta_0 + x_i\beta_1})\bigg) + \sum^{n}_{i=m+1}\bigg( (\beta_0+x_i\beta_1) \mathbb{E}_{\boldsymbol{y_{mis}}}[y_i|\boldsymbol{x, y_{obs}, \beta^{(t)}}] \bigg) \\
    &=\sum^{m}_{i=1}\bigg(y_i(\beta_0+x_i\beta_i)\bigg)-\sum^{n}_{i=1}\bigg(\log(1 + e^{\beta_0 + x_i\beta_1})\bigg) + \sum^{n}_{i=m+1}\bigg( (\beta_0 + x_i\beta_1)p_i(\boldsymbol{\beta}^{(t)}) \bigg) \\
    &(\mathbb{E}(Y_i)=p_i(\boldsymbol{\beta}) \text{ as } Y_i\sim\text{Bernoulli}(p_i(\boldsymbol{\beta})))
  \end{split}
\end{equation}

Now we differentiate $Q$ with respect to $\beta_0$ and $\beta_1$ for the M-Step
\begin{equation}
  \begin{split}
    \frac{d}{d\beta_0}Q(\boldsymbol{\beta}|\boldsymbol{\beta^{(t)}}) &=\sum^{m}_{i=1}y_i - \sum^{n}_{i=1}\bigg(\frac{e^{\beta_0+x_i\beta_1}}{1+e^{\beta_0+x_i\beta_1}}\bigg)\\
    &+\sum^{n}_{i=m+1}\bigg(\frac{e^{\beta_0^{(t)}+x_i\beta_1^{(t)}}}{1+e^{\beta_0^{(t)}+x_i\beta_1^{(t)}}}+x_i\beta_1\frac{e^{\beta_0^{(t)}+x_i\beta_1^{(t)}}}{(1+e^{\beta_0^{(t)}+x_i\beta_1^{(t)}})^2}+\beta_0\frac{e^{\beta_0^{(t)}+x_i\beta_1^{(t)}}}{(1+e^{\beta_0^{(t)}+x_i\beta_1^{(t)}})^2} \bigg)\\
    \frac{d}{d\beta_1}Q(\boldsymbol{\beta}|\boldsymbol{\beta^{(t)}}) &=\sum^{m}_{i=1}y_ix_i - \sum^{n}_{i=1}\bigg(x_i\frac{e^{\beta_0+x_i\beta_1}}{1+e^{\beta_0+x_i\beta_1}}\bigg)\\ 
    &+ \sum^{n}_{i=m+1}\bigg(\beta_0\frac{x_ie^{\beta_0^{(t)}+x_i\beta_1^{(t)}}}{(1+e^{\beta_0^{(t)}+x_i\beta_1^{(t)}})^2}+x_i\frac{e^{\beta_0^{(t)}+x_i\beta_1^{(t)}}}{1+e^{\beta_0^{(t)}+x_i\beta_1^{(t)}}}+x_i\beta_1\frac{x_ie^{\beta_0^{(t)}+x_i\beta_1^{(t)}}}{(1+e^{\beta_0^{(t)}+x_i\beta_1^{(t)}})^2} \bigg)\\
  \end{split}
\end{equation}

However, the solutions of the derivatives have no closed form expressions and thus we need to resort to numerical methods. Before proceeding to the implementation, we first need to preprocess `dataex4` due to the `NA` values by rearranging it.
```{r}

```

We can confirm that the order of `dataex4` is changed where the `NA` values are at the last.
Now we proceed to the implementation of the EM algorithm. In the code, we have used for the stopping criterion as below
$$|\beta_0^{(t+1)}-\beta_0^{(t)}|+|\beta_1^{(t+1)}-\beta_1^{(t)}|<\varepsilon$$ 
```{r warning=FALSE}

```
Using two different methods, `maxLik()` and `optim()` with $\varepsilon=1e-6$, we can see that we both obtained similar results by a small difference ($0.001$). Thus, we conclude that the MLE of $\hat{\beta}_{\text{MLE}}=(\hat{\beta_0}_{\text{MLE}},\hat{\beta_1}_{\text{MLE}})$ is $(0.976, -2.48)$ respectively.

\newpage

# Q5

Consider a random sample $Y_1,...,Y_n$ from the mixture distribution
with density
$$f(y) = pf_{\text{LogNormal}}(y;\mu, \sigma^2) + (1-p)f_{\text{Exp}}(y;\lambda),$$

with \begin{equation*}
  \begin{split}
  f_{\text{LogNormal}}(y;\mu, \sigma^2) &= \frac{1}{y\sqrt{2\pi\sigma^2}}\text{exp}\left\{\frac{1}{2\sigma^2}(\log y - \mu)^2\right\}, 
  \quad y>0, \quad \mu \in \mathbb{R}, \; \sigma > 0 \\
  f_{\text{Exp}}(y;\lambda) &= \lambda e^{-\lambda y}, \quad y \geq 0, \quad \lambda > 0
  \end{split}
\end{equation*}

and $\boldsymbol{\theta} = (p, \mu, \sigma^2, \lambda)$ 

## a) 
Derive the EM algorithm to find the updating equations for $\boldsymbol{\theta^{(t+1)}} = (p^{(t+1)}, \mu^{(t+1)}, (\sigma^{(t+1)})^2, \lambda^{(t+1)})$.

### **Answer** :

Let us consider a mixture model of Log-Normal and Exponential distributions.
$$\mathbb{P}(Y\leq y)=p\cdot\frac{1}{y\sqrt{2\pi\sigma^2}}\text{exp}\left\{\frac{1}{2\sigma^2}(\log y - \mu)^2\right\}+(1-p)\cdot\lambda e^{-\lambda y}$$

Let $z_i$ be the binary latent variables indicating component membership, i.e.

\begin{equation*}
  z_i=
  \begin{cases}
    1 & \text{if } y_i \text{ belong to } f_{\text{LogNormal}}(y;\mu, \sigma^2) \\
    0 & \text{if } y_i \text{ belong to } f_{\text{Exp}}(y;\lambda)
  \end{cases}
\end{equation*}

The observed data in this context is $\boldsymbol{y}=(y_1\dots y_n)$ and the missing data is $\boldsymbol{z}=(z_1\dots z_n)$. The likelihood of the complete data $(\boldsymbol{y,z})$ is
\begin{equation}\label{eq:mixloglik}
  \begin{split}
    L(\theta;\boldsymbol{y,z})&=\prod^{n}_{i=1}\bigg(p\cdot\frac{1}{y_i\sqrt{2\pi\sigma^2}}\text{exp}\left\{\frac{1}{2\sigma^2}(\log y_i - \mu)^2\right\}\bigg)^{z_i}\bigg((1-p)\cdot\lambda e^{-\lambda y_i}\bigg)^{1-z_i} \\
    \implies \log L(\theta;\boldsymbol{y,z})&=\sum^{n}_{i=1}z_i\log\bigg(p\cdot\frac{1}{y_i\sqrt{2\pi\sigma^2}}\text{exp}\left\{\frac{1}{2\sigma^2}(\log y_i - \mu)^2\right\}\bigg)+\sum^{n}_{i=1}(1-z_i)\log\bigg((1-p)\cdot\lambda e^{-\lambda y_i}\bigg)
  \end{split}
\end{equation}

with the corresponding log likelihood (\ref{eq:mixloglik}), we proceed to E-Step,

\begin{equation}\label{eq:qfn}
  \begin{split}
    Q(\boldsymbol{\theta}|\boldsymbol{\theta^{(t)}})&=\mathbb{E}_{Z}(\log L(\theta;\boldsymbol{y,z})|\boldsymbol{y,\theta^{(t)}})\\
    &=\sum^{n}_{i=1}\mathbb{E}(Z_i|y_i,\theta^{(t)})\log\bigg(p\cdot\frac{1}{y_i\sqrt{2\pi\sigma^2}}\text{exp}\left\{\frac{1}{2\sigma^2}(\log y_i - \mu)^2\right\}\bigg)\\
    &+\sum^{n}_{i=1}(1-\mathbb{E}(Z_i|y_i,\theta^{(t)}))\log\bigg((1-p)\cdot\lambda e^{-\lambda y_i}\bigg)
  \end{split}
\end{equation}

We know that $\mathbb{E}(Z_i|\boldsymbol{y,\theta^{(t)}})=\mathbb{P}(Z_i=1|y_i,\theta^{(t)})$, and applying Bayes Theorem and the Law of Total Probability, we obtain,

\begin{equation}\label{eq:sub}
  \begin{split}
    \mathbb{E}(Z_i|\boldsymbol{y,\theta^{(t)}})&=\mathbb{P}(Z_i=1|y_i,\theta^{(t)})\\
    &=\frac{\bigg(p^{(t)}\cdot\frac{1}{y_i\sqrt{2\pi(\sigma^2)^{(t)}}}\text{exp}\left\{\frac{1}{2(\sigma^2)^{(t)}}(\log y_i - \mu^{(t)})^2\right\}\bigg)}{\bigg(p\cdot\frac{1}{y_i\sqrt{2\pi(\sigma^2)^{(t)}}}\text{exp}\left\{\frac{1}{2(\sigma^2)^{(t)}}(\log y_i - \mu^{(t)})^2\right\}\bigg)\bigg((1-p^{(t)})\cdot\lambda^{(t)} e^{-\lambda^{(t)} y_i}\bigg)}\\
    &= \tilde{p}_i^{(t)}, \quad i=1,\dots,n
  \end{split}
\end{equation}
Therefore, we substitute \ref{eq:sub} into \ref{eq:qfn}
\begin{equation}
  \begin{split}
    Q(\boldsymbol{\theta}|\boldsymbol{\theta^{(t)}})&=\sum^{n}_{i=1}\tilde{p}_i^{(t)}\log\bigg(p\cdot\frac{1}{y_i\sqrt{2\pi\sigma^2}}\text{exp}\left\{\frac{1}{2\sigma^2}(\log y_i - \mu)^2\right\}\bigg)\\
    &+\sum^{n}_{i=1}(1-\tilde{p}_i^{(t)})\log\bigg((1-p)\cdot\lambda e^{-\lambda y_i}\bigg)
  \end{split}
\end{equation}
For the M-step, we only need to compute the partial derivatives
\begin{equation}
  \begin{split}
    \frac{\partial}{\partial p} Q(\boldsymbol{\theta}|\boldsymbol{\theta^{(t)}})&=0 \implies p^{(t+1)}=\frac{\sum^{n}_{i=1}\tilde{p}_i^{(t)}}{n}\\
    \frac{\partial}{\partial \mu} Q(\boldsymbol{\theta}|\boldsymbol{\theta^{(t)}})&=0 \implies \mu^{(t+1)} =\frac{\sum^{n}_{i=1}\tilde{p}_i^{(t)}\log(y_i)}{\sum^{n}_{i=1}\tilde{p}_i^{(t)}} \\
    \frac{\partial}{\partial \sigma^2} Q(\boldsymbol{\theta}|\boldsymbol{\theta^{(t)}})&=0 \implies (\sigma^{(t+1)})^2=\frac{\sum^{n}_{i=1}\tilde{p}_i^{(t)}(\log(y_i)-\mu^{(t+1)})^2}{\sum^{n}_{i=1}\tilde{p}_i^{(t)}y_i}\\
    \frac{\partial}{\partial \lambda} Q(\boldsymbol{\theta}|\boldsymbol{\theta^{(t)}})&=0 \implies \lambda^{(t+1)}=\frac{\sum^{n}_{i=1}(1-\tilde{p}_i^{(t)})}{\sum^{n}_{i=1}y_i(1-\tilde{p}_i^{(t)})}
  \end{split}
\end{equation}

\newpage

## b)

Using the dataset `datasetex5.Rdata` implement the EM algorithm and find
the MLEs for each component of $\theta$. As starting values, you might
want to consider $\theta^{(0)} = (p^{(0)}, \mu^{0)}, (\sigma^{(0)})^2, \lambda^{(0)}) = (0.1, 1, 0.5^2, 2)$. Draw the histogram of the data with the estimated density superimposed.

### **Answer** :

In the code, we have used for the stopping criterion
$$|p^{(t+1)}-p^{(t)}|+|\mu^{(t+1)-\mu{(t)}}|+|(\sigma^{(t+1)})^2-(\sigma^{(t)})^2|+|\lambda^{(t+1)}-\lambda^{(t)}|<\varepsilon$$
with $\varepsilon=0.00001$. For the starting values we use $\theta^{(0)} = (p^{(0)}, \mu^{0)}, (\sigma^{(0)})^2, \lambda^{(0)}) = (0.1, 1, 0.5^2, 2)$ as given.


```{r}

```

```{r}

```

Using the **R** code provided, we obtained $\hat{p}=0.480,\quad\hat{\mu}=2.01,\quad\hat{\sigma}^2=0.864,\quad\hat{\lambda}=1.03$. Also, we further performed the nonparametric bootstrap to quantify uncertainty about the parameters as shown on the table above. The plot of the observed counts against the expected counts under this mixture model is shown below. We can see that the generated mixture model represents the dataset well.
```{r}

```