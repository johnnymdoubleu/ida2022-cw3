---
title: "IDA Assignment 3"
author: "Johnny Lee, s1687781"
date: "8th April 2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setlocale("LC_ALL", "English")
library(maxLik)
library(mice)
library(dplyr)
library(knitr)
library(kableExtra)

load("dataex2.Rdata")
load("dataex4.Rdata")
load("NHANES2.Rdata")
```

# Q1.
Consider the `nhanes` dataset in mice. For more information please type help(`nhanes`) in the R console.

## a)
(2 marks) What percentage of the cases is incomplete?

### **Answer** :

```{r}
cat("The percentage of incomplete case is",
    (nrow(nhanes)-nrow(cc(nhanes)))*100/nrow(nhanes))
```

\newpage

## b)
(4 marks) Impute the data with \texttt{mice} using the defaults with \texttt{seed=1}, in step 2 predict \texttt{bmi} from \texttt{age}, \texttt{hyp}, and \texttt{chl} by the normal linear regression model, and then pool the results. What are the proportions of variance due to the missing data for each parameter? Which parameters appear to be most affected by the nonresponse?

### **Answer** :

```{r}
pool1 <- pool(with(mice(nhanes, printFlag = F, seed = 1), lm(bmi ~ age + hyp + chl)))
kable(pool1$pooled[,c(1,3,7,8,10)], caption="Imputation with seed=1") %>%
  kable_styling(latex_options = "hold_position")
```

\newpage

## c)
(4 marks) Repeat the analysis for $\texttt{seed} \in \{2,3,4,5,6\}$. Do the conclusions remain the same?

```{r}
pool2 <- pool(with(mice(nhanes, printFlag = F, seed = 2), lm(bmi ~ age + hyp + chl)))
pool3 <- pool(with(mice(nhanes, printFlag = F, seed = 3), lm(bmi ~ age + hyp + chl)))
pool4 <- pool(with(mice(nhanes, printFlag = F, seed = 4), lm(bmi ~ age + hyp + chl)))
pool5 <- pool(with(mice(nhanes, printFlag = F, seed = 5), lm(bmi ~ age + hyp + chl)))
pool6 <- pool(with(mice(nhanes, printFlag = F, seed = 6), lm(bmi ~ age + hyp + chl)))

parameters <- c("(Intercept)", "age", "hyp", "chl")
df <- data.frame(parameters, pool2$pooled[,10], pool3$pooled[,10],
                 pool4$pooled[,10], pool5$pooled[,10], pool6$pooled[,10])
colnames(df) <- c("parameters", "seed=2", "seed=3", "seed=4", "seed=5", "seed=6")
kable(df, caption="Imputation with seed=2,3,4,5,6") %>%
  kable_styling(latex_options = "hold_position")
```

\newpage

### **Answer** :

## d)
(4 marks) Repeat the analysis with $M = 100$ with the same seeds. Would you prefer these analyses over those with $M = 5$? Explain why.

```{r}
pool1 <- pool(with(mice(nhanes, m = 100, printFlag = F, seed = 1), 
                   lm(bmi ~ age + hyp + chl)))
pool2 <- pool(with(mice(nhanes, m = 100, printFlag = F, seed = 2), 
                   lm(bmi ~ age + hyp + chl)))
pool3 <- pool(with(mice(nhanes, m = 100, printFlag = F, seed = 3), 
                   lm(bmi ~ age + hyp + chl)))
pool4 <- pool(with(mice(nhanes, m = 100, printFlag = F, seed = 4), 
                   lm(bmi ~ age + hyp + chl)))
pool5 <- pool(with(mice(nhanes, m = 100, printFlag = F, seed = 5), 
                   lm(bmi ~ age + hyp + chl)))
pool6 <- pool(with(mice(nhanes, m = 100, printFlag = F, seed = 6), 
                   lm(bmi ~ age + hyp + chl)))

parameters <- c("(Intercept)", "age", "hyp", "chl")
df <- data.frame(parameters, pool1$pooled[,10], pool2$pooled[,10],
                 pool3$pooled[,10], pool4$pooled[,10], 
                 pool5$pooled[,10], pool6$pooled[,10])
colnames(df) <- c("parameters", "seed=1", "seed=2", "seed=3", "seed=4",
                  "seed=5", "seed=6")
kable(df, caption="Imputation with seed=1,2,3,4,5,6 and M=100") %>%
  kable_styling(latex_options = "hold_position")
```

The pooled estimates is more stable when the value of $M$ is higher. Thus, we would prefer $M=100$ than $M=5$

\newpage

# Q2.
(15 marks) Each of the 100 datasets contained in the file dataex2.Rdata was generated
in the following way 
$$y_i|x_i\overset{\text{ind.}}{\sim} \text{N}(\beta_0+\beta_1x_i,1), \quad x_i\overset{\text{ind.}}{\sim}\text{Unif}(-1,1), \quad \beta_0=1, \quad \beta_1=3$$
for $i = 1, \dots, 100$. Additionally, some of the responses were set to be missing using a MAR mechanism. The goal of this exercise is to study the effect that acknowledging/not acknowledging parameter uncertainty when performing step $1$ of multiple imputation might have on the coverage of the corresponding confidence intervals. Further suppose that the analysis of interest in step $2$ is to fit the regression model that was used to generate the data, i.e., a normal linear regression model where the response is $y$ and the covariate is $x$. With the aid of the \texttt{mice} package, calculate the empirical coverage probability of the $95\%$ confidence intervals for $\beta_1$ under the following two approaches: stochastic regression imputation and the corresponding bootstrap based version. Comment. For both approaches, please consider $m = 20$ and \texttt{seed=1}. \textbf{NOTE 1:} In order to calculate the empirical coverage probability, you only need to compute the proportion of the time (over the 100 intervals) that the interval contains the true value of the parameter. \textbf{NOTE 2:} The data are stored in an array structure such that, for instance, \texttt{dataex2[, , 1]}, corresponds to the first dataset (which has $100$ rows and $2$ columns, with the first column containing the values of $x$ and the second the values of $y$).

```{r}
# initialize a counter
count <- 0
for (i in 1:nrow(dataex2)) {
  #impute values for the ith dataset using M=20
  impute.sri <- mice(dataex2[, , i], m = 20, method = "norm.nob", printFlag = F, seed = 1)
  fit.sri <- with(impute.sri, lm(Y ~ X)) #step 2
  pool.sri <- pool(fit.sri) # step 3
  summary.sri <- summary(pool.sri, conf.int = TRUE)
  if (summary.sri[2, 7] <= 3 & summary.sri[2, 8] >= 3) {
  count <- count + 1 #add to the counter if the the value of beta1 is contained in the
  #confidence interval
  }
}
ecp.sri <- count/nrow(dataex2)
cat("the proportion of the time for Stochastic Imputation is", ecp.sri)
```

```{r}
# initialize a counter
count <- 0
for (i in 1:nrow(dataex2)) {
  #impute values for the ith dataset, using m=20
  impute.bootstrap <- mice(dataex2[,,i], m = 20, method = "norm.boot",
                           printFlag = FALSE, seed = 1)
  fit.bootstrap <- with(impute.bootstrap, lm(Y ~ X)) #step 2
  pool.bootstrap <- pool(fit.bootstrap) # step 3
  summary.bootstrap <- summary(pool.bootstrap, conf.int = TRUE)
  if (summary.bootstrap[2, c(7)] <= 3 & summary.bootstrap[2, c(8)] >= 3) {
    count = count + 1 #add to the counter if the true value of beta1 is contained in the
  }
}
ecp.bootstrap <- count/nrow(dataex2)
cat("the proportion of the time for Bootstrap is", ecp.bootstrap)
```

\newpage

# Q3.
(9 marks) Show that for a linear (in the coefficients) regression model, the following two strategies coincide:

(i) Computing the predicted values (point estimates) from each fitted model in step $2$ and then pooling them according to Rubin’s rule for point estimates (i.e., averaging the predicted values across the imputed datasets).

(ii) Pooling the regression coefficients from each fitted model in step $2$ using Rubin’s rule for point estimates and then computing the predicted values afterwards


### **Answer** :

We consider a linear regression model given a dataset as $\{y_i,x_{1i},\dots,x_{ni}\}$
\begin{equation*}
  \begin{split}
    y_i=\beta_0 + \beta_1x_{1i} + \cdots + \beta_nx_{ni} + \varepsilon_i,\quad\varepsilon_i\sim N(0,\sigma^2)
  \end{split}
\end{equation*}

Now we look into Case (i), we compute the predicted values for each fitted model from step $2$. Then we obtain as below, 
\begin{equation*}
  \begin{split}
    \hat{y}^{(m)}_i=\hat{\beta}^{(m)}_0 + \hat{\beta}^{(m)}_1x_{1i} + \cdots + \hat{\beta}^{(m)}_nx_{ni}
  \end{split}
\end{equation*}

Then we pool them according to Rubin's rule for point estimates.
\begin{equation*}
  \begin{split}
    \Bar{y}_i&=\frac{1}{M}\sum^{M}_{i=1}\hat{y}^{(m)}\\
    &=\frac{1}{M}\sum^{M}_{i=1}\bigg(\hat{\beta}^{(m)}_0 + \hat{\beta}^{(m)}_1x_{1i} + \cdots + \hat{\beta}^{(m)}_nx_{ni}\bigg) \\
    &= \frac{1}{M}\sum^{M}_{i=1}\hat{\beta}^{(m)}_0+\frac{1}{M}\sum^{M}_{i=1}\hat{\beta}^{(m)}_1x_{1i}+\cdots+\frac{1}{M}\sum^{M}_{i=1}\hat{\beta}^{(m)}_nx_{ni}\\
    &=\Bar{\beta_0}+\Bar{\beta_1}x_{1i}+\cdots+\Bar{\beta_n}x_{ni}
  \end{split}
\end{equation*}

Now, let us consider Case (ii) to validate if they coincide. We pool the regression coefficients from each fitted model in step $2$ using Rubin's rule for point estimates.
\begin{equation*}
  \begin{split}
    \Bar{\beta}_0 &=\frac{1}{M}\sum^{M}_{i=1}\hat{\beta}^{(m)}_0 \\
    &\vdots \\
    \Bar{\beta}_n &=\frac{1}{M}\sum^{M}_{i=1}\hat{\beta}^{(m)}_n
  \end{split}
\end{equation*}
Then we can compute the predicted values as follow
\begin{equation*}
  \begin{split}
    \Bar{y}_i&=\Bar{\beta_0}+\Bar{\beta_1}x_{1i}+\cdots+\Bar{\beta_n}x_{ni}
  \end{split}
\end{equation*}

As shown above, the order of the computation of predicted values for each fitted model in step $2$ and pooling according to Rubin's rule for point estimates do not matter mathematically. Therefore, we conclude here by saying that both cases coincide.
\newpage

# Q4.

The goal of this exercise is to study different ways of using \texttt{mice} when the analysis model of interest/substantive model involves an interaction term between incomplete variables. The model used to generate the data (available in \texttt{dataex4.Rdata}), which corresponds to our model of interest in step $2$, was the following one:
$$y_i=\beta_0+\beta_1x_{1i}+\beta_2x_{2i}+\beta_3x_{1i}x_{2i}+\varepsilon_i,\\ x_{1i}\overset{\text{iid}}{\sim}\text{N}(0,1), \quad x_{2i}\overset{\text{iid}}{\sim}\text{N}(1.5,1), \quad
\varepsilon_i\overset{\text{iid}}{\sim}\text{N}(0,1)$$

for $i = 1,\dots,1000, \beta_0 = 1.5, \beta_1 = 1, \beta_2 = 2$ and $\beta_3 = 1$. Additionally, missingness was imposed on $y$ and $x_1$ and so the interaction variable $x_1x_2$ also has missing values, although the missingness in this interaction variable is induced by the missing in the covariate $x_1$. In the following, please use $M = 50$ and \texttt{seed=1}.

## a)
(6 marks) By only imputing the y and x1 variables in step $1$, provide the estimates of $\beta_1, \beta_2$, and $\beta_3$ along with $95\%$ confidence intervals. Comment. Note that this approach where the interaction variable is left outside the imputation process and calculated afterwards in the analysis model, is known as \textit{Impute, then transform}

### **Answer** :

```{r}
head(dataex4)
```

\newpage

## b)
(10 marks) Now, start by calculating the interaction variable in the incomplete data and append it as a variable to your dataset. Then, use \textit{passive imputation} to impute the interaction variable. Provide the estimates of $\beta_1, \beta_2$, and $\beta_3$ along with $95\%$ confidence intervals. Comment.

### **Answer** :

\newpage

## c)
(10 marks) Now that you have already appended the interaction variable to the dataset, impute it as it was just another variable (or like any other variable) in the dataset and use this variable for the interaction term in step $2$. Provide the estimates of $\beta_1, \beta_2$ and $\beta_3$ along with $95\%$ confidence intervals. Comment.

### **Answer** :

\newpage

## d)
(6 marks) What is the obvious conceptual drawback of the \textit{just another variable} approach for imputing interactions?

### **Answer** :

\newpage

# Q5
(30 marks) The file `NHANES2.Rdata` contains a subset of data from the \textit{National Health and Nutrition Examination Survey} (NHANES), whose goal is to assess the health and nutritional status of adults and children in the United States. The variables in the dataset are the following:
\begin{itemize}
  \item \texttt{wgt}: weight in kg,
  \item \texttt{gender}: male vs female,
  \item \texttt{bili}: bilirubin concentration in mg/dL,
  \item \texttt{age}: in years,
  \item \texttt{chol}: total serum cholestrol in mg/dL,
  \item \texttt{HDL}: High-density lipoprotein cholestrol in mg/dL,
  \item \texttt{hgt}: height in metres,
  \item \texttt{educ}: educational status; 5 ordered categories,
  \item \texttt{race}: 5 unordered categories,
  \item \texttt{SBP}: systolic blood pressure in mmHg,
  \item \texttt{hypten}: hyptertensive status; binary,
  \item \texttt{WC}: waist circumference in cm.
\end{itemize}
The analysis of interest is the following:
$$\text{wgt}=\beta_0+\beta_1\text{gender}+\beta_2\text{age}+\beta_3\text{hgt}+\beta_4\text{WC}+\varepsilon,\quad\varepsilon\sim N(0,\sigma^2).$$
Using multiple imputation and conducting all necessary checks, report your findings.

### **Answer** :

```{r}

```

