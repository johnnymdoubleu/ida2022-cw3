---
title: "IDA Assignment 3"
author: "Johnny Lee, s1687781"
date: "8th April 2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setlocale("LC_ALL", "English")
library(maxLik)
library(mice)
library(dplyr)
library(knitr)
library(kableExtra)
library(JointAI)
library(RColorBrewer)
require(devtools)
source_url("https://gist.githubusercontent.com/NErler/0d00375da460dd33839b98faeee2fdab/raw/c6f537ecf80eddcefd94992ec7926aa57d454536/propplot.R")

load("dataex2.Rdata")
load("dataex4.Rdata")
load("NHANES2.Rdata")
```

# Q1.
Consider the `nhanes` dataset in mice. For more information please type help(`nhanes`) in the R console.

## a)
(2 marks) What percentage of the cases is incomplete?

### **Answer** :

```{r}
cat("The percentage of incomplete cases is",
    (nrow(nhanes)-nrow(cc(nhanes)))*100/nrow(nhanes))
```

\newpage

## b)
(4 marks) Impute the data with \texttt{mice} using the defaults with \texttt{seed=1}, in step 2 predict \texttt{bmi} from \texttt{age}, \texttt{hyp}, and \texttt{chl} by the normal linear regression model, and then pool the results. What are the proportions of variance due to the missing data for each parameter? Which parameters appear to be most affected by the nonresponse?

### **Answer** :

```{r}
#data preprocessing
nhanes$age <- as.factor(nhanes$age)
nhanes$hyp <- as.factor(nhanes$hyp)
```


```{r}
pool1 <- pool(with(mice(nhanes, printFlag = F, seed = 1), 
                   lm(bmi ~ age + hyp + chl)))
kable(pool1$pooled[,c(1,3,7,8,10)], caption = "Imputation with seed=1") %>%
  kable_styling(latex_options = "hold_position")
```

From the lecture notes, we know that to estimate the variance of $\hat{\theta}^{\text{MI}}$, we need to compute the between-imputation variance, \textbf{B} and within-imputation variance, $\bar{\textbf{U}}$. 
$$
\text{B}=\frac{1}{M-1}\sum^{M}_{i=1}(\hat{\theta}^{(i)}-\hat{\theta}^{\text{MI}})^2
$$
$$\bar{\text{U}}=\frac{1}{M}\sum^{m}_{i=1}\hat{\text{U}}^{(i)}$$

With the above statistics, we can calculate the total variance, $V^{\text{MI}}=\bar{U}+(1+\frac{1}{M})B$. Since `mice()` provides a different ratio known as $\lambda$ where $\lambda=\frac{B+\frac{B}{N}}{V^{\text{MI}}}$. In this setup, `bmi` is the $\theta$.

Now to perform the above statistics, we first took `age` and `hyp` as categorical variables and assigned them to be factors. From Table 1, we computed the pooled estimates, degree of freedom and lambda values. This `lambda` represents the proportions of variance due to the missing data for each parameter. We can observe that`age` factors generally have higher $\lambda$ values than the others. Especially, `age(2)` had the highest value of $0.6107359$ and `hyp(2)` had the lowest value of $0.2635386$. We need to note that the intercept contains `age(1)` and `hyp(1)` and having the $\lambda$ value of $0.3225123$. Hence, we conclude that `age` is mostly affected by the nonresponse.


\newpage

## c)
(4 marks) Repeat the analysis for $\texttt{seed} \in \{2,3,4,5,6\}$. Do the conclusions remain the same?

### **Answer** :

```{r}
#Compute pooled estimates for each seed
pool2 <- pool(with(mice(nhanes, printFlag = F, seed = 2), 
                   lm(bmi ~ age + hyp + chl)))
pool3 <- pool(with(mice(nhanes, printFlag = F, seed = 3), 
                   lm(bmi ~ age + hyp + chl)))
pool4 <- pool(with(mice(nhanes, printFlag = F, seed = 4), 
                   lm(bmi ~ age + hyp + chl)))
pool5 <- pool(with(mice(nhanes, printFlag = F, seed = 5), 
                   lm(bmi ~ age + hyp + chl)))
pool6 <- pool(with(mice(nhanes, printFlag = F, seed = 6), 
                   lm(bmi ~ age + hyp + chl)))
#tabulating the values we obtained from mice()
parameters <- c("(Intercept)", "age(2)", "age(3)","hyp(2)", "chl")
df <- data.frame(parameters, pool2$pooled[,10], pool3$pooled[,10],
                 pool4$pooled[,10], pool5$pooled[,10], pool6$pooled[,10])
colnames(df) <- c("parameters", "seed=2", "seed=3", "seed=4", "seed=5", "seed=6")
kable(df, caption = "Imputation with seed=2,3,4,5,6") %>%
  kable_styling(latex_options = "hold_position")
```

Here we repeated the same step as Q1 b) with different seed. From Table 2, we can see that the most affected variables will change with different number of settings. For instance, in `seed=2`, `hyp(2)` has the highest $\lambda$ value of $0.5160375$ whereas `seed=3,4 and 5` have `age` factors as the most affected variable. Thus, this is unstable throughout different seed values because the percentage of imputation we found in Q1 a) is considerably high with $48\%$.

\newpage

## d)
(4 marks) Repeat the analysis with $M = 100$ with the same seeds. Would you prefer these analyses over those with $M = 5$? Explain why.

### **Answer** :

```{r}
#compute the pooled estimates from each seed with M=100
pool1 <- pool(with(mice(nhanes, m = 100, printFlag = F, seed = 1), 
                   lm(bmi ~ age + hyp + chl)))
pool2 <- pool(with(mice(nhanes, m = 100, printFlag = F, seed = 2), 
                   lm(bmi ~ age + hyp + chl)))
pool3 <- pool(with(mice(nhanes, m = 100, printFlag = F, seed = 3), 
                   lm(bmi ~ age + hyp + chl)))
pool4 <- pool(with(mice(nhanes, m = 100, printFlag = F, seed = 4), 
                   lm(bmi ~ age + hyp + chl)))
pool5 <- pool(with(mice(nhanes, m = 100, printFlag = F, seed = 5), 
                   lm(bmi ~ age + hyp + chl)))
pool6 <- pool(with(mice(nhanes, m = 100, printFlag = F, seed = 6), 
                   lm(bmi ~ age + hyp + chl)))
#tabulating the values from mice()
parameters <- c("(Intercept)", "age(2)", "age(3)","hyp(2)", "chl")
df <- data.frame(parameters, pool1$pooled[,10], pool2$pooled[,10],
                 pool3$pooled[,10], pool4$pooled[,10], 
                 pool5$pooled[,10], pool6$pooled[,10])
colnames(df) <- c("parameters", "seed=1", "seed=2", "seed=3", "seed=4",
                  "seed=5", "seed=6")
kable(df, caption = "Imputation with seed=1,2,3,4,5,6 and M=100") %>%
  kable_styling(latex_options = "hold_position")
```

In Table 3, we computed the $\lambda$ and we can notice that $\lambda$ became more stable when the value of $M$ is higher. Also, we can observe that either `age(2)` or `age(3)` have the highest $\lambda$ throughout different seeds. Thus, we would prefer $M=100$ over $M=5$. This is also evident from the equation in Q1 b) where, if $M$ is high, we can reduce the values for $\bar{\text{U}}$ and $B$ and the total variance, $V^{\text{MI}}$. $M=100$ is also a reasonable choice for the size of this dataset as the number of observation is $25$. Hence, the time efficiency will not be affected significantly.

\newpage

# Q2.
(15 marks) Each of the 100 datasets contained in the file dataex2.Rdata was generated
in the following way 
$$y_i|x_i\overset{\text{ind.}}{\sim} \text{N}(\beta_0+\beta_1x_i,1), \quad x_i\overset{\text{ind.}}{\sim}\text{Unif}(-1,1), \quad \beta_0=1, \quad \beta_1=3$$
for $i = 1, \dots, 100$. Additionally, some of the responses were set to be missing using a MAR mechanism. The goal of this exercise is to study the effect that acknowledging/not acknowledging parameter uncertainty when performing step $1$ of multiple imputation might have on the coverage of the corresponding confidence intervals. Further suppose that the analysis of interest in step $2$ is to fit the regression model that was used to generate the data, i.e., a normal linear regression model where the response is $y$ and the covariate is $x$. With the aid of the \texttt{mice} package, calculate the empirical coverage probability of the $95\%$ confidence intervals for $\beta_1$ under the following two approaches: stochastic regression imputation and the corresponding bootstrap based version. Comment. For both approaches, please consider $m = 20$ and \texttt{seed=1}. \textbf{NOTE 1:} In order to calculate the empirical coverage probability, you only need to compute the proportion of the time (over the 100 intervals) that the interval contains the true value of the parameter. \textbf{NOTE 2:} The data are stored in an array structure such that, for instance, \texttt{dataex2[, , 1]}, corresponds to the first dataset (which has $100$ rows and $2$ columns, with the first column containing the values of $x$ and the second the values of $y$).

### **Answer** :

```{r}
count <- 0 # initialize a counter
for (i in 1:nrow(dataex2)) {
  #perform SRI with m=20
  impute.sri <- mice(dataex2[, , i], m = 20, method = "norm.nob", printFlag = F, seed = 1)
  fit.sri <- with(impute.sri, lm(Y ~ X)) #step 2
  pool.sri <- pool(fit.sri) # step 3
  summary.sri <- summary(pool.sri, conf.int = TRUE)
  if (summary.sri[2, 7] <= 3 & summary.sri[2, 8] >= 3) {
  #increment the count if beta1 is contained in the confidence interval
    count <- count + 1
  }
}
cat("the empirical coverage probability for Stochastic Imputation is", count/nrow(dataex2))
```

```{r}
# initialize a counter
count <- 0
for (i in 1:nrow(dataex2)) {
  #perform bootstrap imputation with m=20
  impute.bootstrap <- mice(dataex2[,,i], m = 20, method = "norm.boot",
                           printFlag = FALSE, seed = 1)
  fit.bootstrap <- with(impute.bootstrap, lm(Y ~ X)) #step 2
  pool.bootstrap <- pool(fit.bootstrap) # step 3
  summary.bootstrap <- summary(pool.bootstrap, conf.int = TRUE)
  if (summary.bootstrap[2, c(7)] <= 3 & summary.bootstrap[2, c(8)] >= 3) {
    #increment the count if beta1 is contained in the confidence interval
    count = count + 1
  }
}
cat("the empirical coverage probability for Bootstrap is", count/nrow(dataex2))
```

We performed both stochastic imputation and bootstrap imputation to compute the empirical coverage probability. As a result we obtained $0.88$ and $0.95$ respectively. The reason behind this is that SRI does not take the variability of the function weight into account. This can result in the missing consideration on the uncertainty of the imputed values. Thus, we will have an improper multiple imputation and the confidence intervals based on the total variance can be narrow. For the bootstrap imputation, it is the proper multiple imputation and thus the confidence interval will be wider.

\newpage

# Q3.
(9 marks) Show that for a linear (in the coefficients) regression model, the following two strategies coincide:

(i) Computing the predicted values (point estimates) from each fitted model in step $2$ and then pooling them according to Rubin’s rule for point estimates (i.e., averaging the predicted values across the imputed datasets).

(ii) Pooling the regression coefficients from each fitted model in step $2$ using Rubin’s rule for point estimates and then computing the predicted values afterwards


### **Answer** :

We consider a linear regression model given a dataset as $\{y_i,x_{1i},\dots,x_{ni}\}$
\begin{equation*}
  \begin{split}
    y_i=\beta_0 + \beta_1x_{1i} + \cdots + \beta_nx_{ni} + \varepsilon_i,\quad\varepsilon_i\sim N(0,\sigma^2), \quad i=1,\dots,n
  \end{split}
\end{equation*}

Now we look into Case (i), we compute the predicted values for each fitted model from step $2$. Then we obtain as below, 
\begin{equation*}
  \begin{split}
    \hat{y}^{(m)}_i=\hat{\beta}^{(m)}_0 + \hat{\beta}^{(m)}_1x_{1i} + \cdots + \hat{\beta}^{(m)}_nx_{ni}, \quad i=1,\dots,n
  \end{split}
\end{equation*}

Then we pool them according to Rubin's rule for point estimates.
\begin{equation*}
  \begin{split}
    \Bar{y}_i&=\frac{1}{M}\sum^{M}_{i=1}\hat{y}^{(m)}\\
    &=\frac{1}{M}\sum^{M}_{i=1}\bigg(\hat{\beta}^{(m)}_0 + \hat{\beta}^{(m)}_1x_{1i} + \cdots + \hat{\beta}^{(m)}_nx_{ni}\bigg) \\
    &= \frac{1}{M}\sum^{M}_{i=1}\hat{\beta}^{(m)}_0+\frac{1}{M}\sum^{M}_{i=1}\hat{\beta}^{(m)}_1x_{1i}+\cdots+\frac{1}{M}\sum^{M}_{i=1}\hat{\beta}^{(m)}_nx_{ni}\\
    &=\Bar{\beta_0}+\Bar{\beta_1}x_{1i}+\cdots+\Bar{\beta_n}x_{ni}, \quad i=1,\dots,n
  \end{split}
\end{equation*}

Now, let us consider Case (ii) to validate if they coincide. We pool the regression coefficients from each fitted model in step $2$ using Rubin's rule for point estimates.
\begin{equation*}
  \begin{split}
    \Bar{\beta}_0 &=\frac{1}{M}\sum^{M}_{i=1}\hat{\beta}^{(m)}_0 \\
    &\vdots \\
    \Bar{\beta}_n &=\frac{1}{M}\sum^{M}_{i=1}\hat{\beta}^{(m)}_n
  \end{split}
\end{equation*}
Then we can compute the predicted values as follow
\begin{equation*}
  \begin{split}
    \Bar{y}_i&=\Bar{\beta_0}+\Bar{\beta_1}x_{1i}+\cdots+\Bar{\beta_n}x_{ni}, \quad i=1,\dots,n
  \end{split}
\end{equation*}

As shown above, the order of the computation of predicted values for each fitted model in step $2$ and pooling according to Rubin's rule for point estimates do not matter mathematically. Therefore, we conclude here by saying that both cases coincide.
\newpage

# Q4.

The goal of this exercise is to study different ways of using \texttt{mice} when the analysis model of interest/substantive model involves an interaction term between incomplete variables. The model used to generate the data (available in \texttt{dataex4.Rdata}), which corresponds to our model of interest in step $2$, was the following one:
$$y_i=\beta_0+\beta_1x_{1i}+\beta_2x_{2i}+\beta_3x_{1i}x_{2i}+\varepsilon_i,\\ x_{1i}\overset{\text{iid}}{\sim}\text{N}(0,1), \quad x_{2i}\overset{\text{iid}}{\sim}\text{N}(1.5,1), \quad
\varepsilon_i\overset{\text{iid}}{\sim}\text{N}(0,1)$$

for $i = 1,\dots,1000$, $\beta_0 = 1.5$, $\beta_1 = 1$, $\beta_2 = 2$ and $\beta_3 = 1$. Additionally, missingness was imposed on $y$ and $x_1$ and so the interaction variable $x_1x_2$ also has missing values, although the missingness in this interaction variable is induced by the missing in the covariate $x_1$. In the following, please use $M = 50$ and \texttt{seed=1}.

```{r}
kable(head(dataex4), caption = "first 6 values of dataex4")  %>%
  kable_styling(latex_options = "hold_position")
```


## a)
(6 marks) By only imputing the $y$ and $x_1$ variables in step $1$, provide the estimates of $\beta_1, \beta_2$, and $\beta_3$ along with $95\%$ confidence intervals. Comment. Note that this approach where the interaction variable is left outside the imputation process and calculated afterwards in the analysis model, is known as \textit{Impute, then transform}

### **Answer** :

```{r}
#perform sri with m=50
impute.sri <- mice(dataex4, m = 50, seed = 1, printFlag = FALSE)
fit.sri <- with(impute.sri, lm(y ~ x1 + x2 + x1*x2)) #step 2
pool.sri <- pool(fit.sri) #step 3
kable(summary(pool.sri, conf.int = TRUE)[, c(1,2,3,7,8)], 
      caption = "Summary Statistics of Imputation of $y$ and $x_1$") %>%
  kable_styling(latex_options = "hold_position")
```

From Table 4, we can observe the missingness of the data. Looking at Table 5 now, the estimated coefficient of $x_2$, the confidence interval contains the true values. On the other hand, for $x_1$ and $x_1x_2$ the true value is not contained within the confidence interval. This is because, there is no missing values for $x_2$ but in $x_1$. The corresponding $x_1x_2$ will also contain the missing value due to $x_1$. 

\newpage

## b)
(10 marks) Now, start by calculating the interaction variable in the incomplete data and append it as a variable to your dataset. Then, use \textit{passive imputation} to impute the interaction variable. Provide the estimates of $\beta_1, \beta_2$, and $\beta_3$ along with $95\%$ confidence intervals. Comment.

### **Answer** :

```{r}
#store each columns
x1 <- dataex4$x1; x2 <- dataex4$x2; dataex4$x1x2 <- x1*x2
#perform the null imputaton with baseline
impute.null <- mice(dataex4, maxit = 0)
method <- impute.null$method
#specify formula to calculate x1x2
method["x1x2"] <- "~I(x1*x2)"
pred <- impute.null$predictorMatrix
pred[c("x1", "x2"), "x1x2"] <- 0
#change visiting scheme
visit.seq <- impute.null$visitSequence
visit.seq
```

```{r}
#performing passive imputation with m=50
impute.passive <- mice(dataex4, method = method, predictorMatrix = pred, 
                visitSequence = visit.seq, m = 50, seed = 1, printFlag = FALSE)
pool.passive <- pool(with(impute.passive, lm(y ~ x1 + x2 + x1*x2))) #step 2, 3
kable(summary(pool.passive, conf.int = TRUE)[,c(1,2,3,7,8)],
      caption = "Summary Statistics of Imputation of $y$ and $x_1$") %>%
  kable_styling(latex_options = "hold_position")
```
We performed \textit{passive imputation} and presented the `estimates`, `std.error` and the confidence intervals and showed in Table 6. As Q4 a), only $x_2$ contains the true value within the confidence interval whereas $x_1$ and $x_1x_2$ do not contain. However, we need to note that the estimates are closer to the confidence interval compared to the previous part.


\newpage

## c)
(10 marks) Now that you have already appended the interaction variable to the dataset, impute it as it was \textit{just another variable} (or like any other variable) in the dataset and use this variable for the interaction term in step $2$. Provide the estimates of $\beta_1, \beta_2$ and $\beta_3$ along with $95\%$ confidence intervals. Comment.

### **Answer** :

```{r}
#performing just anotehr variable imputation with m=50
impute.jav <- mice(dataex4, m = 50, seed = 1, printFlag = FALSE)
fit.jav <- with(impute.jav, lm(y ~ x1 + x2 + x1x2)) #step 2
pool.jav <- pool(fit.jav) #step 3
kable( summary(pool.jav, conf.int = TRUE)[, c(1,2,3,7,8)],
      caption = "Summary Statistics of Imputation of $y$ and $x_1$") %>%
  kable_styling(latex_options = "hold_position")
```

We performed \textit{just another variable} imputation and presented the results in Table 7. This time, we observe that all confidence intervals of the coefficients include the true values $\beta_0 = 1.5$, $\beta_1 = 1$, $\beta_2 = 2$ and $\beta_3 = 1$.

\newpage

## d)
(6 marks) What is the obvious conceptual drawback of the \textit{just another variable} approach for imputing interactions?

### **Answer** :

The conceptual drawback of the \textit{just another variable} approach for imputing interaction is the imputation on $x_1x_2$ is not using $x_1$ and $x_2$ from the observed dataset. Thus, the product of $x_1$ and $x_2$ is not equal to $x_1x_2$. As a result, this will result in the biasness as the unbiased estimator for the parameters from the regression will no longer hold.

\newpage

# Q5
(30 marks) The file `NHANES2.Rdata` contains a subset of data from the \textit{National Health and Nutrition Examination Survey} (NHANES), whose goal is to assess the health and nutritional status of adults and children in the United States. The variables in the dataset are the following:
\begin{itemize}
  \item \texttt{wgt}: weight in kg,
  \item \texttt{gender}: male vs female,
  \item \texttt{bili}: bilirubin concentration in mg/dL,
  \item \texttt{age}: in years,
  \item \texttt{chol}: total serum cholestrol in mg/dL,
  \item \texttt{HDL}: High-density lipoprotein cholestrol in mg/dL,
  \item \texttt{hgt}: height in metres,
  \item \texttt{educ}: educational status; 5 ordered categories,
  \item \texttt{race}: 5 unordered categories,
  \item \texttt{SBP}: systolic blood pressure in mmHg,
  \item \texttt{hypten}: hyptertensive status; binary,
  \item \texttt{WC}: waist circumference in cm.
\end{itemize}
The analysis of interest is the following:
$$\text{wgt}=\beta_0+\beta_1\text{gender}+\beta_2\text{age}+\beta_3\text{hgt}+\beta_4\text{WC}+\varepsilon,\quad\varepsilon\sim N(0,\sigma^2).$$
Using multiple imputation and conducting all necessary checks, report your findings.

### **Answer** :

```{r fig.height=4}
nhanes2 <- NHANES2
#compute number of oberservation per pattern
md_pattern(nhanes2, pattern = FALSE, color = c('#34111b', '#e30f41'))
```

```{r fig.height=4}
par(mar = c(3,3,2,1), mgp = c(2,0.6,0))
#visaulise the distribution of each feature
plot_all(nhanes2, breaks = 30, ncol = 4)
```

```{r}
#compute dry set up
impute.null <- mice(nhanes2, maxit = 0)
#change default imputation method
meth <- impute.null$method
meth["hgt"] <- "norm"
meth
post <- impute.null$post
#specifying the range of hgt
post["hgt"] <- "imp[[j]][,i] <- squeeze(imp[[j]][,i], c(1.397, 1.9304))"
```
While inspecting the missing data pattern, we found $411$ observations with observed values on all $12$ variables. Also, $10$ observations for which on `WC` is missing, $6$ observations for which on `hgt` is missing Interestingly, we can observe that there are $29$ observations for which on `chol`, `HDL` and `bili` are missing.

We can visualise the variables' distributions using `plot_all()`. In the plot above, depicting the distribution of the observed data for the different variables, we could appreciate that `hgt` following a normal distribution is possibly not a completely unreasonable idea. Let us then change the default from predictive mean matching method (`pmm`) to `norm` for the variable `hgt`.

However, we need to be careful, because we do not want to risk imputing a negative value for the height. With the below syntax all imputed values of `hgt` that are outside the interval `(min(nhanes2$hgt), max(nhanes2$hgt))` will be set to those limiting values.

Since our model is limited to this, $\text{wgt}=\beta_0+\beta_1\text{gender}+\beta_2\text{age}+\beta_3\text{hgt}+\beta_4\text{WC}+\varepsilon,\quad\varepsilon\sim N(0,\sigma^2).$. We will end our data preprocessing and proceed to the next them in tuning the hyperparameters for imputation

```{r}
#compute the pooled estimates with different seed and M values
seed1.5 <- pool(with(mice(nhanes2, methods = meth, post = post,
                          m = 5, seed = 1, printFlag = FALSE),
                     lm(wgt ~ gender + age + hgt + WC)))
seed1.10 <- pool(with(mice(nhanes2, methods = meth, post = post,
                          m = 10, seed = 1, printFlag = FALSE),
                     lm(wgt ~ gender + age + hgt + WC)))
seed1.20 <- pool(with(mice(nhanes2, methods = meth, post = post,
                          m = 20, seed = 1, printFlag = FALSE),
                     lm(wgt ~ gender + age + hgt + WC)))
seed1.25 <- pool(with(mice(nhanes2, methods = meth, post = post,
                          m = 25, seed = 1, printFlag = FALSE),
                     lm(wgt ~ gender + age + hgt + WC)))

seed2.5 <- pool(with(mice(nhanes2, methods = meth, post = post, maxit = 20,
                          m = 5, seed = 2, printFlag = FALSE),
                     lm(wgt ~ gender + age + hgt + WC)))
seed2.10 <- pool(with(mice(nhanes2, methods = meth, post = post, maxit = 20,
                          m = 10, seed = 2, printFlag = FALSE),
                     lm(wgt ~ gender + age + hgt + WC)))
seed2.20 <- pool(with(mice(nhanes2, methods = meth, post = post, maxit = 20,
                          m = 20, seed = 2, printFlag = FALSE),
                     lm(wgt ~ gender + age + hgt + WC)))
seed2.25 <- pool(with(mice(nhanes2, methods = meth, post = post, maxit = 20,
                          m = 25, seed = 2, printFlag = FALSE),
                     lm(wgt ~ gender + age + hgt + WC)))
```

```{r}
kable(data.frame(summary(seed1.5, conf.int = TRUE)[, c(1, 2, 3, 7, 8)], 
                 lambda = seed1.5$pooled[,10]), caption = "Seed=1 and m=5") %>%
  kable_styling(latex_options = "hold_position")
kable(data.frame(summary(seed1.10, conf.int = TRUE)[, c(1, 2, 3, 7, 8)], 
                 lambda = seed1.10$pooled[,10]), caption = "Seed=1 and m=10") %>%
  kable_styling(latex_options = "hold_position")
kable(data.frame(summary(seed1.20, conf.int = TRUE)[, c(1, 2, 3, 7, 8)], 
                 lambda = seed1.20$pooled[,10]), caption = "Seed=1 and m=20") %>%
  kable_styling(latex_options = "hold_position")
kable(data.frame(summary(seed1.25, conf.int = TRUE)[, c(1, 2, 3, 7, 8)], 
                 lambda = seed1.25$pooled[,10]), caption = "Seed=1 and m=25") %>%
  kable_styling(latex_options = "hold_position")
kable(data.frame(summary(seed2.5, conf.int = TRUE)[, c(1, 2, 3, 7, 8)], 
                 lambda = seed2.5$pooled[,10]), caption = "Seed=2 and m=5") %>%
  kable_styling(latex_options = "hold_position")
kable(data.frame(summary(seed2.10, conf.int = TRUE)[, c(1, 2, 3, 7, 8)], 
                 lambda = seed2.10$pooled[,10]), caption = "Seed=2 and m=10") %>%
  kable_styling(latex_options = "hold_position")
kable(data.frame(summary(seed2.20, conf.int = TRUE)[, c(1, 2, 3, 7, 8)], 
                 lambda = seed2.20$pooled[,10]), caption = "Seed=2 and m=20") %>%
  kable_styling(latex_options = "hold_position")
kable(data.frame(summary(seed2.25, conf.int = TRUE)[, c(1, 2, 3, 7, 8)], 
                 lambda = seed2.25$pooled[,10]), 
      caption = "Seed=2 and m=25") %>%
  kable_styling(latex_options = "hold_position")

```
Looking at Tables above, we have the (pooled) estimates, standard errors, the confident intervals and lambda which get more stable as M increases and we can be more confident in any one specific run. Since we run the multiple imputation with a sufficiently large $M$, the results will with high probability only differ by a small amount. Thus we will choose `seed=1` with `M=25` and proceed to the multiple imputation.

```{r}
#imputation with the chosen seed and M
imp <- mice(nhanes2, method = meth, post = post, maxit = 20, 
            m = 25, seed = 1, printFlag = FALSE)
imp$loggedEvents
```
We can also confirm through `loggedEvents` that no problems occurred during the imputation.
```{r warning=FALSE, fig.height=4}
plot(imp, layout=c(4,4))
bwplot(imp)[c(2,4,5,6,7,8)]
densityplot(imp)
densityplot(imp,~hgt|gender, xlim = c(1.2, 2.2))
densityplot(imp,~hgt|hypten, xlim = c(1.2, 2.2))
densityplot(imp,~SBP|hypten + gender, xlim=c(50, 180))
stripplot(imp)[6]
propplot(imp)
```

Now let us process the plots one by one to check if the `mice()` converged. Except for `educ`, rest of the variables are continuous are we proceed to the first plot. The reason for a missing plot for `educ` is because there is only one missing value `educ` so no standard deviation. The traceplot indicates that the iterative algorithm appears to have converged for all variables that were imputed. The next plots are boxplots and density plots for the observed and imputed ones where we label them as blue and red respectively. 

Note that we are using `M=25` and because the density of the observed data is possibly plotted first, we can barely see it. However, we can generally confirm that most imputed datasets follow a similar distribution. The most outstanding plots are the ones from `SBP` and `hgt` as there is a shift towards lower values.

 Specifically, we investigated with `hgt` conditional on the `gender` and `hgt` conditional on `hypten`. We can see that to a certain extent, both plots explain the differences between the observed and imputed values for `hgt`. This is because `male` and `positive` have much narrower distribution compared to wider distribution for `female` and `negative`. For `SBP` we could observe similarity. Using `stripplot()` we can double confirm that the imputation seems reasonable.

We observe an abnormal imputation pattern for the `educ` variable however this
is not a cause for concern since we are only imputing  a single missing
value out of 500 cases. Meanwhile, for the `hypten` variable, we observe
a reasonable amount between imputation variance but not enough such that the 
general distribution of this variable is lost. In summary, all imputations
for variables with missing data have been performed successfully.

We can compare the distribution of the imputed values against the distribution of the observed categorical variables' values using `propplot()`. This compares the proportion of values in each category. This shows a large discrepancy between the observed and imputed data distributions for the `educ`.
However educ only contains $1$ missing variable and we can conclude that there is insufficient evidence to say that the imputation `educ` has problem. For the `hypten`, there is a reasonable proportion between the imputation variances.

All together, we can confirm that the multiple imputation is successful and proceed to analysis of the imputed data. Since we set `M=25`, we will only look at the first case.

```{r}
#fitting with given parameters
fit <- with(imp, lm(wgt ~ gender + age + hgt + WC))
summary(fit$analyses[[1]])
```

Looking at the summary we obtained the estimates as below,
$$
\text{wgt}=-100.83781-1.36542\times\text{gender} -0.15556\times \text{age}+52.42865\times \text{hgt} + 1.02524\times\text{WC}
$$
We can also check that only the coefficient for `genderfemale` has a p-value greater than $0.05$. To this, we will check using Wald Test later for the importance of it.
```{r fig.height=3.7}
plot(fit$analyses[[1]]$fitted.values, residuals(fit$analyses[[1]]),
     xlab = "Fitted values", ylab = "Residuals")
```
In this fitted values versus residuals plot we can observe that the points are spread equally and randomly. Thus, no obvious trend and we can believe that the linear assumption holds for this model. 

```{r fig.height=3.7}
qqnorm(rstandard(fit$analyses[[1]]), xlim = c(-4, 4), ylim = c(-6, 6))
qqline(rstandard(fit$analyses[[1]]), col = 2)
```
By observing the normal Q-Q plot, we do not see the deviance from the red dotted base line. Therefore, we can conclude that the data after imputation follows normal distribution and the normality assumption holds. Lastly, we will proceed to pooling the estimates with our imputed dataset. From Table 16, the summary statistics is provided for the pooled estimates.
```{r}
pooled_ests <- pool(fit)
kable(summary(pooled_ests, conf.int = TRUE)[c(1,2,6,7,8)], 
      caption = "Summary Statistics") %>%
  kable_styling(latex_options = "hold_position")
```

```{r}
kable(pool.r.squared(pooled_ests, adjusted = TRUE), caption = "") %>%
  kable_styling(latex_options = "hold_position")
```
Looking at the Table 17, the adjusted $R^2$ values indicates good fit.

Now we conduct Wald Test to check the importance of the features.

```{r}
#performing wald test
fit.no.WC <- with(imp, lm(wgt ~ gender + age + hgt ))
fit.no.hgt<- with(imp, lm(wgt ~ gender + age + WC))
fit.no.age <- with(imp, lm(wgt ~ gender + hgt + WC))
fit.no.gender <- with(imp, lm(wgt ~ age + hgt + WC))
wald.stats <- rbind(D1(fit, fit.no.WC)$result, D1(fit, fit.no.hgt)$result, 
            D1(fit, fit.no.age)$result, D1(fit, fit.no.gender)$result)
excluded.features <- c("WC", "hgt", "age", "gender")
df <- data.frame(excluded.features, wald.stats)
kable(df, caption = "Wald Test on each features") %>%
  kable_styling(latex_options = "hold_position")
```
We conducted the Wald test by excluding each feature one by one and used `D1()` to compute its value. From the Table, we can see that `gender` is the only feature having p-value greater than $0.05$. Although it is above $0.05$, we can still consider its important but limiting it into $0.1$ significance level. Thus, `WC`, `hgt` and `age` have more influence than `gender` here.